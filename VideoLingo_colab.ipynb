{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VideoLingo Colab (批处理 / 无 WebUI)\n",
    "\n",
    "本 Notebook 用于在 Colab 的 T4 GPU 上按步骤运行 VideoLingo 的批处理模式（不使用 Streamlit WebUI）。\n",
    "输出结果会保存在 `batch/output/`。\n",
    "\n",
    "准备：LLM/ASR/TTS 的 API Key（若使用云端服务）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 选择 T4 GPU\n",
    "在 Colab 菜单：`Runtime -> Change runtime type -> Hardware accelerator` 选择 **T4 GPU**。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 克隆仓库\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash -lc 'if [ -d /content/VideoLingo ]; then cd /content/VideoLingo && git pull; else git clone https://github.com/MAE5blog/VideoLingo.git /content/VideoLingo; fi'\n",
    "%cd /content/VideoLingo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 安装依赖（约 5-10 分钟）\n",
    "包括系统依赖（ffmpeg/字体）与 Python 依赖，并安装 GGUF 翻译模型所需的 `llama_cpp`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update -y\n",
    "!apt-get install -y ffmpeg fonts-noto-cjk libsndfile1 build-essential cmake ninja-build\n",
    "\n",
    "!python -m pip install -U pip\n",
    "!grep -vE '^torch' requirements.txt > /tmp/requirements_no_torch.txt\n",
    "!python -m pip install -r /tmp/requirements_no_torch.txt\n",
    "\n",
    "# 兜底安装：requirements 里可能因网络失败导致缺包\n",
    "!python -m pip install json-repair\n",
    "\n",
    "# 兜底安装：WhisperX 依赖（ctranslate2/faster-whisper）\n",
    "!python -m pip install ctranslate2==4.4.0 tokenizers>=0.14.0\n",
    "# 如果上面的 ctranslate2 安装失败，可启用下面这行预编译包\n",
    "# !python -m pip install https://oplist.mae5.com/d/gdrive_lz26xg/share/ctranslate2-4.4.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl?sign=s08ZZHeakHFTTevv8Vja5I-6HPXyT4ojOHMesEXpZUQ=:0\n",
    "\n",
    "# 兜底安装：WhisperX（本地 ASR），避免 PyAV 编译失败（py3.12）\n",
    "!python -m pip install av==12.0.0\n",
    "!python -m pip install faster-whisper==1.0.0 --no-deps\n",
    "!python -m pip install git+https://github.com/m-bain/whisperx.git@7307306a9d8dd0d261e588cc933322454f853853 --no-deps\n",
    "!python -m pip install pyannote.audio==3.1.1\n",
    "\n",
    "# 兜底安装：字幕时长估计依赖\n",
    "!python -m pip install syllables pypinyin g2p-en\n",
    "\n",
    "# 兜底安装：字幕对齐/纠错依赖\n",
    "!python -m pip install autocorrect-py\n",
    "\n",
    "# 兜底安装：Demucs（人声分离）\n",
    "!python -m pip install -U \"demucs>=4.0.1\"\n",
    "\n",
    "# torchaudio 新版本保存音频可能需要 torchcodec\n",
    "!python -m pip install torchcodec\n",
    "\n",
    "# 修复 torchaudio 缺失 set_audio_backend（Python 3.12）\n",
    "import site\n",
    "from pathlib import Path\n",
    "patch = \"\"\"try:\n",
    "    import torchaudio\n",
    "    import sys\n",
    "    import types\n",
    "    if not hasattr(torchaudio, 'set_audio_backend'):\n",
    "        def _noop_backend(_backend):\n",
    "            return None\n",
    "        torchaudio.set_audio_backend = _noop_backend\n",
    "    if not hasattr(torchaudio, 'get_audio_backend'):\n",
    "        torchaudio.get_audio_backend = lambda: 'soundfile'\n",
    "    if not hasattr(torchaudio, 'list_audio_backends'):\n",
    "        torchaudio.list_audio_backends = lambda: ['soundfile']\n",
    "    backend_mod = sys.modules.get('torchaudio.backend')\n",
    "    if backend_mod is None:\n",
    "        backend_mod = types.ModuleType('torchaudio.backend')\n",
    "        sys.modules['torchaudio.backend'] = backend_mod\n",
    "    if not hasattr(backend_mod, '__path__'):\n",
    "        backend_mod.__path__ = []\n",
    "    backend_mod.set_audio_backend = torchaudio.set_audio_backend\n",
    "    backend_mod.get_audio_backend = torchaudio.get_audio_backend\n",
    "    backend_mod.list_audio_backends = torchaudio.list_audio_backends\n",
    "    if 'torchaudio.backend.common' not in sys.modules:\n",
    "        common_mod = types.ModuleType('torchaudio.backend.common')\n",
    "        common_mod.set_audio_backend = torchaudio.set_audio_backend\n",
    "        common_mod.get_audio_backend = torchaudio.get_audio_backend\n",
    "        common_mod.list_audio_backends = torchaudio.list_audio_backends\n",
    "        sys.modules['torchaudio.backend.common'] = common_mod\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    if not hasattr(np, 'NaN'):\n",
    "        np.NaN = np.nan\n",
    "except Exception:\n",
    "    pass\"\"\"\n",
    "dst = Path(site.getsitepackages()[0]) / 'sitecustomize.py'\n",
    "dst.write_text(patch)\n",
    "print('sitecustomize patched:', dst)\n",
    "\n",
    "# 同步修补当前内核，避免还没重启就报错\n",
    "try:\n",
    "    import torchaudio\n",
    "    import sys\n",
    "    import types\n",
    "    if not hasattr(torchaudio, 'set_audio_backend'):\n",
    "        torchaudio.set_audio_backend = lambda *_args, **_kwargs: None\n",
    "    if not hasattr(torchaudio, 'get_audio_backend'):\n",
    "        torchaudio.get_audio_backend = lambda: 'soundfile'\n",
    "    if not hasattr(torchaudio, 'list_audio_backends'):\n",
    "        torchaudio.list_audio_backends = lambda: ['soundfile']\n",
    "    backend_mod = sys.modules.get('torchaudio.backend')\n",
    "    if backend_mod is None:\n",
    "        backend_mod = types.ModuleType('torchaudio.backend')\n",
    "        sys.modules['torchaudio.backend'] = backend_mod\n",
    "    if not hasattr(backend_mod, '__path__'):\n",
    "        backend_mod.__path__ = []\n",
    "    backend_mod.set_audio_backend = torchaudio.set_audio_backend\n",
    "    backend_mod.get_audio_backend = torchaudio.get_audio_backend\n",
    "    backend_mod.list_audio_backends = torchaudio.list_audio_backends\n",
    "    if 'torchaudio.backend.common' not in sys.modules:\n",
    "        common_mod = types.ModuleType('torchaudio.backend.common')\n",
    "        common_mod.set_audio_backend = torchaudio.set_audio_backend\n",
    "        common_mod.get_audio_backend = torchaudio.get_audio_backend\n",
    "        common_mod.list_audio_backends = torchaudio.list_audio_backends\n",
    "        sys.modules['torchaudio.backend.common'] = common_mod\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    import numpy as np\n",
    "    if not hasattr(np, 'NaN'):\n",
    "        np.NaN = np.nan\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# llama_cpp server 依赖\n",
    "!python -m pip install fastapi uvicorn sse-starlette pydantic-settings requests-toolbelt\n",
    "\n",
    "# 预编译 llama-cpp-python wheel（优先下载，失败则源码编译）\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "LLAMA_WHEEL_URL = \"https://oplist.mae5.com/d/gdrive_lz26xg/share/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl?sign=s08ZZHeakHFTTevv8Vja5I-6HPXyT4ojOHMesEXpZUQ=:0\"\n",
    "LLAMA_WHEEL_DIR = Path(\"/content/llama_wheels\")\n",
    "LLAMA_WHEEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LLAMA_WHEEL_PATH = LLAMA_WHEEL_DIR / \"llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl\"\n",
    "if not LLAMA_WHEEL_PATH.exists():\n",
    "    try:\n",
    "        print(\"Downloading prebuilt llama-cpp-python wheel ...\")\n",
    "        urllib.request.urlretrieve(LLAMA_WHEEL_URL, LLAMA_WHEEL_PATH)\n",
    "    except Exception as e:\n",
    "        print(\"Prebuilt wheel download failed, will build from source:\", e)\n",
    "\n",
    "!bash -lc 'LLAMA_WHEEL_DIR=/content/llama_wheels; mkdir -p \"$LLAMA_WHEEL_DIR\"; if ls \"$LLAMA_WHEEL_DIR\"/llama_cpp_python-*.whl >/dev/null 2>&1; then python -m pip install \"$LLAMA_WHEEL_DIR\"/llama_cpp_python-*.whl; else MAX_JOBS=2 CMAKE_ARGS=\"-DGGML_CUDA=on -DCMAKE_CUDA_ARCHITECTURES=75\" FORCE_CMAKE=1 python -m pip wheel llama-cpp-python -w \"$LLAMA_WHEEL_DIR\" -v; python -m pip install \"$LLAMA_WHEEL_DIR\"/llama_cpp_python-*.whl; fi'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 配置 `config.yaml`\n",
    "只需要修改下面代码块里的变量。\n",
    "\n",
    "默认已按：**日语 → 简体中文**、**本地 Kotoba-Whisper + Sakura GGUF**、**不配音**。\n",
    "\n",
    "- 翻译服务会在需要时自动启动（每一步完成后卸载模型并清理显存）。\n",
    "- GGUF 模型会在首次翻译步骤自动下载到 `_model_cache/llm`。\n",
    "- 如果只做字幕，把任务表里的 `Dubbing` 设为 `0`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "try:\n",
    "    from ruamel.yaml import YAML\n",
    "except ModuleNotFoundError:\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'ruamel.yaml'])\n",
    "    from ruamel.yaml import YAML\n",
    "\n",
    "# ====== 需要你填写的配置 ======\n",
    "LLM_API_KEY = \"local\"\n",
    "LLM_BASE_URL = \"http://127.0.0.1:8000\"\n",
    "LLM_MODEL = \"SakuraLLM/Sakura-7B-Qwen2.5-v1.0-GGUF@sakura-7b-qwen2.5-v1.0-iq4xs.gguf\"\n",
    "LLM_SUPPORT_JSON = False\n",
    "MAX_WORKERS = 1\n",
    "SUMMARY_LENGTH = 2000\n",
    "\n",
    "TARGET_LANGUAGE = \"简体中文\"\n",
    "SOURCE_LANGUAGE = \"ja\"\n",
    "WHISPER_RUNTIME = \"local\"  # \"local\" 或 \"cloud\"\n",
    "WHISPER_MODEL = \"kotoba-tech/kotoba-whisper-v2.2\"\n",
    "WHISPER_302_API_KEY = \"YOUR_302_API_KEY\"\n",
    "\n",
    "USE_DEMUCS = True\n",
    "BURN_SUBTITLES = True\n",
    "YTB_RESOLUTION = \"1080\"  # 360/1080/best\n",
    "\n",
    "# 本地翻译服务（llama_cpp + GGUF）\n",
    "LOCAL_LLM_ENABLED = True\n",
    "LOCAL_LLM_MANAGE = True\n",
    "LOCAL_LLM_MODEL_REPO = \"SakuraLLM/Sakura-7B-Qwen2.5-v1.0-GGUF\"\n",
    "LOCAL_LLM_MODEL_FILE = \"sakura-7b-qwen2.5-v1.0-iq4xs.gguf\"\n",
    "LOCAL_LLM_MODEL_DIR = \"./_model_cache/llm\"\n",
    "LOCAL_LLM_MODEL_PATH = \"\"\n",
    "LOCAL_LLM_MODEL_ALIAS = LLM_MODEL\n",
    "LOCAL_LLM_SERVER_HOST = \"127.0.0.1\"\n",
    "LOCAL_LLM_SERVER_PORT = 8000\n",
    "LOCAL_LLM_API_KEY = \"local\"\n",
    "LOCAL_LLM_N_GPU_LAYERS = -1\n",
    "LOCAL_LLM_N_CTX = 4096\n",
    "LOCAL_LLM_N_THREADS = 8\n",
    "LOCAL_LLM_N_BATCH = 512\n",
    "LOCAL_LLM_CHAT_FORMAT = \"\"\n",
    "LOCAL_LLM_LOG_PATH = \"output/log/local_llm_server.log\"\n",
    "\n",
    "# 配音（可选）\n",
    "TTS_METHOD = \"edge_tts\"  # azure_tts/openai_tts/fish_tts/gpt_sovits 等\n",
    "EDGE_VOICE = \"zh-CN-XiaoxiaoNeural\"\n",
    "# ============================\n",
    "\n",
    "yaml = YAML()\n",
    "cfg_path = Path(\"config.yaml\")\n",
    "with cfg_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    cfg = yaml.load(f)\n",
    "\n",
    "cfg[\"api\"][\"key\"] = LLM_API_KEY\n",
    "cfg[\"api\"][\"base_url\"] = LLM_BASE_URL\n",
    "cfg[\"api\"][\"model\"] = LLM_MODEL\n",
    "cfg[\"api\"][\"llm_support_json\"] = LLM_SUPPORT_JSON\n",
    "cfg[\"max_workers\"] = MAX_WORKERS\n",
    "cfg[\"summary_length\"] = SUMMARY_LENGTH\n",
    "\n",
    "cfg[\"target_language\"] = TARGET_LANGUAGE\n",
    "cfg[\"whisper\"][\"language\"] = SOURCE_LANGUAGE\n",
    "cfg[\"whisper\"][\"detected_language\"] = SOURCE_LANGUAGE\n",
    "cfg[\"whisper\"][\"runtime\"] = WHISPER_RUNTIME\n",
    "cfg[\"whisper\"][\"model\"] = WHISPER_MODEL\n",
    "cfg[\"whisper\"][\"whisperX_302_api_key\"] = WHISPER_302_API_KEY\n",
    "\n",
    "cfg[\"demucs\"] = USE_DEMUCS\n",
    "cfg[\"burn_subtitles\"] = BURN_SUBTITLES\n",
    "cfg[\"ytb_resolution\"] = YTB_RESOLUTION\n",
    "\n",
    "cfg.setdefault(\"local_llm\", {})\n",
    "cfg[\"local_llm\"][\"enabled\"] = LOCAL_LLM_ENABLED\n",
    "cfg[\"local_llm\"][\"manage_server\"] = LOCAL_LLM_MANAGE\n",
    "cfg[\"local_llm\"][\"model_repo\"] = LOCAL_LLM_MODEL_REPO\n",
    "cfg[\"local_llm\"][\"model_file\"] = LOCAL_LLM_MODEL_FILE\n",
    "cfg[\"local_llm\"][\"model_dir\"] = LOCAL_LLM_MODEL_DIR\n",
    "cfg[\"local_llm\"][\"model_path\"] = LOCAL_LLM_MODEL_PATH\n",
    "cfg[\"local_llm\"][\"model_alias\"] = LOCAL_LLM_MODEL_ALIAS\n",
    "cfg[\"local_llm\"][\"server_host\"] = LOCAL_LLM_SERVER_HOST\n",
    "cfg[\"local_llm\"][\"server_port\"] = LOCAL_LLM_SERVER_PORT\n",
    "cfg[\"local_llm\"][\"api_key\"] = LOCAL_LLM_API_KEY\n",
    "cfg[\"local_llm\"][\"n_gpu_layers\"] = LOCAL_LLM_N_GPU_LAYERS\n",
    "cfg[\"local_llm\"][\"n_ctx\"] = LOCAL_LLM_N_CTX\n",
    "cfg[\"local_llm\"][\"n_threads\"] = LOCAL_LLM_N_THREADS\n",
    "cfg[\"local_llm\"][\"n_batch\"] = LOCAL_LLM_N_BATCH\n",
    "cfg[\"local_llm\"][\"chat_format\"] = LOCAL_LLM_CHAT_FORMAT\n",
    "cfg[\"local_llm\"][\"log_path\"] = LOCAL_LLM_LOG_PATH\n",
    "\n",
    "cfg[\"tts_method\"] = TTS_METHOD\n",
    "cfg[\"edge_tts\"][\"voice\"] = EDGE_VOICE\n",
    "\n",
    "with cfg_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    yaml.dump(cfg, f)\n",
    "\n",
    "print(\"config.yaml 已更新\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 准备任务\n",
    "\n",
    "默认用**外链下载**视频（自动保存文件名到 `batch/input/`）。也可改为本地上传或 Google Drive。\n",
    "\n",
    "- 外链：在下方设置 `VIDEO_URL`\n",
    "- 本地视频：上传到 `batch/input/`，任务表里填写文件名（不含路径）\n",
    "- YouTube：任务表里直接填写链接\n",
    "\n",
    "大文件建议先挂载 Google Drive，再复制到 `batch/input/`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认外链下载（留空则跳过）\n",
    "VIDEO_URL = \"https://oplist.mae5.com/d/本地存储/1.mp4?sign=b58L3c5JYGAMNwLcO09asS9CV8aHTlGBXiO3Yi8Pe0Y=:0\"\n",
    "OUT_PATH = \"\"  # 可选：自定义保存文件名\n",
    "\n",
    "import os\n",
    "import urllib.parse\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "def guess_filename(url: str) -> str:\n",
    "    parts = urllib.parse.urlsplit(url)\n",
    "    name = Path(urllib.parse.unquote(parts.path)).name\n",
    "    return name or \"video.mp4\"\n",
    "\n",
    "DOWNLOADED_FILE = \"\"\n",
    "if VIDEO_URL:\n",
    "    os.makedirs(\"batch/input\", exist_ok=True)\n",
    "    filename = OUT_PATH or guess_filename(VIDEO_URL)\n",
    "    out_path = Path(\"batch/input\") / filename\n",
    "    if not out_path.exists():\n",
    "        with requests.get(VIDEO_URL, stream=True, timeout=60) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(out_path, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "    DOWNLOADED_FILE = out_path.name\n",
    "    print(\"Downloaded:\", out_path)\n",
    "else:\n",
    "    print(\"VIDEO_URL 为空，跳过外链下载\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "os.makedirs(\"batch/input\", exist_ok=True)\n",
    "uploaded = files.upload()\n",
    "for name in uploaded:\n",
    "    shutil.move(name, f\"batch/input/{name}\")\n",
    "\n",
    "if uploaded:\n",
    "    DOWNLOADED_FILE = list(uploaded.keys())[0]\n",
    "print(\"Uploaded:\", list(uploaded.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果使用 Google Drive，可运行下面代码挂载，然后手动复制文件到 `batch/input/`：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建/更新任务表 `batch/tasks_setting.xlsx`：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "VIDEO_FILE = globals().get(\"DOWNLOADED_FILE\") or \"your_video.mp4\"  # 也可以写 YouTube 链接\n",
    "\n",
    "tasks = [\n",
    "    {\n",
    "        \"Video File\": VIDEO_FILE,\n",
    "        \"Source Language\": \"ja\",\n",
    "        \"Target Language\": \"简体中文\",\n",
    "        \"Dubbing\": 0,  # 0=不配音, 1=配音\n",
    "        \"Status\": \"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(tasks, columns=[\"Video File\", \"Source Language\", \"Target Language\", \"Dubbing\", \"Status\"])\n",
    "df.to_excel(\"batch/tasks_setting.xlsx\", index=False)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 运行批处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!PYTHONPATH=. python batch/utils/batch_processor.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 导出结果\n",
    "结果会在 `batch/output/`，可以打包下载。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -la batch/output\n",
    "!zip -r videolingo_output.zip batch/output\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"videolingo_output.zip\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}